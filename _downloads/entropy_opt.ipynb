{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy optimization using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import some things from PyTorch that we'll need to perform this optimization. We'll use PyTorch's optimization framework to create a distribution with our desired entropy.\n",
    "\n",
    "- We'll rely on some functions from the root `torch` module.\n",
    "- We'll use the `nn` module to create an `nn.Param` to store the distribution itself.\n",
    "- Finally, for compatibility with a Python language server, I find it's nice to import `Tensor` so that I can use it in type hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a custom loss function that we can use to bring our distribution's entropy closer to our target entropy. A simple way to do this is (with gradients enabled):\n",
    "\n",
    "- Normalize the distribution to sum to 1.\n",
    "- Calculate the entropy of the distribution.\n",
    "- Use a distance metric such as Mean Squared Error to compare the distribution's entropy to the target entropy.\n",
    "\n",
    "Despite its simplicity, this works shockingly well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEAgainstEntropyLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, dist: Tensor, true_entropy: Tensor) -> Tensor:\n",
    "        \n",
    "        # normalize the distribution\n",
    "        dist = torch.softmax(dist, dim=0)\n",
    "        \n",
    "        # calculate the entropy\n",
    "        approx_entropy = -(dist * dist.log()).sum()\n",
    "        \n",
    "        # calculate the mean squared error\n",
    "        mse = (approx_entropy - true_entropy) ** 2\n",
    "        \n",
    "        # return the loss tensor\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define a function that returns a distribution with the desired entropy (or as close as we can get). It will take the following arguments:\n",
    "\n",
    "- `criterion (nn.Module)`: The loss function to use to optimize the entropy (in this case, it will just be the class we defined above).\n",
    "- `support_size (int)`: The number of outcomes we want our random variable to have.\n",
    "- `desired_entropy (float)`: The entropy we want our distribution to have.\n",
    "- `lr (float)`: The learning rate for the optimization algorithm.\n",
    "- `tol (float)`: The tolerance for the optimization algorithm. We will stop when the loss is less than this value.\n",
    "- `max_iter (int)`: The maximum number of iterations to run the optimization algorithm.\n",
    "- `do_logging (bool)`: Whether to log the loss during optimization.\n",
    "- `log_freq (int)`: How often to log the loss during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(\n",
    "    criterion: nn.Module,\n",
    "    support_size: int,\n",
    "    desired_entropy: float,\n",
    "    lr: float = 0.001,\n",
    "    tol: float = 1e-6,\n",
    "    max_iter: int = 100_000,\n",
    "    do_logging: bool = True,\n",
    "    log_freq: int = 200\n",
    ") -> Tensor:\n",
    "    \n",
    "    # define a parameter (gradient updates possible) with the right support size\n",
    "    dist = nn.Parameter(torch.randn((support_size,), dtype=torch.float64))\n",
    "    dist.requires_grad = True\n",
    "    \n",
    "    # make a torch.Tensor with the desired entropy to compute loss\n",
    "    DE = torch.tensor(desired_entropy, dtype=torch.float64)\n",
    "    DE.requires_grad = False\n",
    "    \n",
    "    # define an optimizer over the parameter\n",
    "    optimizer = torch.optim.AdamW([dist], lr=lr)\n",
    "    \n",
    "    i = 0\n",
    "    if do_logging:\n",
    "        print('-----------------------------------------------------')\n",
    "    while True:\n",
    "        \n",
    "        # optimize the parameter\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(dist, DE)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log the loss\n",
    "        if (i % log_freq == 0):\n",
    "            loss_val = loss.item()\n",
    "            if do_logging:\n",
    "                print(f'loss: {loss_val:.4}')\n",
    "            if loss_val < tol: # we are done if the loss is small enough\n",
    "                break\n",
    "        i += 1 # count iterations\n",
    "        \n",
    "        # give up if max_iter is reached\n",
    "        if i > max_iter:\n",
    "            msg = 'Optimization did not converge!'\n",
    "            Warning(msg)\n",
    "            break\n",
    "    \n",
    "    # renormalize\n",
    "    final_dist = torch.softmax(dist, dim=0)\n",
    "    \n",
    "    # summary of results\n",
    "    if do_logging:\n",
    "        print('-----------------------------------------------------')\n",
    "        print(f'sum of probabilities (should be 1): {final_dist.sum()}')\n",
    "        approx_entropy = -(final_dist * final_dist.log()).sum()\n",
    "        print('-----------------------------------------------------')\n",
    "        print(f'desired entropy:    {desired_entropy}')\n",
    "        print(f'true entropy:       {approx_entropy.item()}')\n",
    "        print('-----------------------------------------------------')\n",
    "    \n",
    "    return final_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "loss: 0.3188\n",
      "loss: 0.2091\n",
      "loss: 0.1094\n",
      "loss: 0.04938\n",
      "loss: 0.02052\n",
      "loss: 0.008044\n",
      "loss: 0.002983\n",
      "loss: 0.001043\n",
      "loss: 0.0003462\n",
      "loss: 0.0001118\n",
      "loss: 3.726e-05\n",
      "loss: 1.406e-05\n",
      "loss: 6.524e-06\n",
      "loss: 3.784e-06\n",
      "loss: 2.589e-06\n",
      "loss: 1.94e-06\n",
      "loss: 1.515e-06\n",
      "loss: 1.204e-06\n",
      "loss: 9.639e-07\n",
      "loss: 7.749e-07\n",
      "loss: 6.247e-07\n",
      "loss: 5.048e-07\n",
      "loss: 4.086e-07\n",
      "loss: 3.313e-07\n",
      "loss: 2.69e-07\n",
      "loss: 2.186e-07\n",
      "loss: 1.778e-07\n",
      "loss: 1.448e-07\n",
      "loss: 1.179e-07\n",
      "loss: 9.614e-08\n",
      "loss: 7.841e-08\n",
      "loss: 6.398e-08\n",
      "loss: 5.223e-08\n",
      "loss: 4.265e-08\n",
      "loss: 3.483e-08\n",
      "loss: 2.846e-08\n",
      "loss: 2.326e-08\n",
      "loss: 1.901e-08\n",
      "loss: 1.554e-08\n",
      "loss: 1.271e-08\n",
      "loss: 1.039e-08\n",
      "loss: 8.501e-09\n",
      "loss: 6.954e-09\n",
      "loss: 5.689e-09\n",
      "loss: 4.654e-09\n",
      "loss: 3.808e-09\n",
      "loss: 3.116e-09\n",
      "loss: 2.55e-09\n",
      "loss: 2.087e-09\n",
      "loss: 1.708e-09\n",
      "loss: 1.398e-09\n",
      "loss: 1.144e-09\n",
      "loss: 9.368e-10\n",
      "loss: 7.668e-10\n",
      "loss: 6.277e-10\n",
      "loss: 5.139e-10\n",
      "loss: 4.207e-10\n",
      "loss: 3.444e-10\n",
      "loss: 2.82e-10\n",
      "loss: 2.308e-10\n",
      "loss: 1.89e-10\n",
      "loss: 1.547e-10\n",
      "loss: 1.267e-10\n",
      "loss: 1.037e-10\n",
      "loss: 8.493e-11\n",
      "loss: 6.954e-11\n",
      "loss: 5.694e-11\n",
      "loss: 4.662e-11\n",
      "loss: 3.817e-11\n",
      "loss: 3.126e-11\n",
      "loss: 2.559e-11\n",
      "loss: 2.096e-11\n",
      "loss: 1.716e-11\n",
      "loss: 1.405e-11\n",
      "loss: 1.151e-11\n",
      "loss: 9.422e-12\n",
      "loss: 7.715e-12\n",
      "loss: 6.318e-12\n",
      "loss: 5.173e-12\n",
      "loss: 4.237e-12\n",
      "loss: 3.469e-12\n",
      "loss: 2.841e-12\n",
      "loss: 2.327e-12\n",
      "loss: 1.905e-12\n",
      "loss: 1.56e-12\n",
      "loss: 1.278e-12\n",
      "loss: 1.047e-12\n",
      "loss: 8.572e-13\n",
      "-----------------------------------------------------\n",
      "sum of probabilities (should be 1): 1.0\n",
      "-----------------------------------------------------\n",
      "desired entropy:    1.5707963267948966\n",
      "true entropy:       1.5707972521604139\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0203, 0.0098, 0.2433, 0.0118, 0.0225, 0.0226, 0.0166, 0.3209, 0.0217,\n",
       "        0.3105], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dist(\n",
    "    criterion=MSEAgainstEntropyLoss(),\n",
    "    support_size=10,\n",
    "    desired_entropy=1.5707963267948966, # pi/2 because why not\n",
    "    lr=0.001,\n",
    "    tol=1e-12,\n",
    "    max_iter=100_000,\n",
    "    do_logging=True,\n",
    "    log_freq=200\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
