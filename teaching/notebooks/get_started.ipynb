{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 1 - Getting Started with PyTorch on Colab"
      ],
      "metadata": {
        "id": "5Cl9stIAcLX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Colab Usage"
      ],
      "metadata": {
        "id": "By9lnKNLG2xK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab is a service provided by Google which provides access to computing resources (like GPUs) which are useful for training and using neural networks. A Colab notebook consists of:\n",
        "- Code cells which accept:\n",
        " - Ordinary Python code\n",
        " - Unix/Linux style bash commands - these must be preceded by an exclamation mark: `!command-name arguments...`\n",
        " - Notebook-specific \"magic commands\" - these must be preceded by a percent sign: `%command-name arguments...`\n",
        "- Text (markdown) cells"
      ],
      "metadata": {
        "id": "YSySufgtcbzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, we can set the runtime type using the Runtime menu at the top of the notebook:\n",
        "\n",
        "**Runtime > Change runtime type > T4 GPU**\n",
        "\n",
        "will give you access to a cheap GPU. Feel free to do this."
      ],
      "metadata": {
        "id": "0t0uzORJEsXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also check your CPU, Disk, and GPU memory usage in the upper right corner; click the box that says RAM and Disk."
      ],
      "metadata": {
        "id": "-qIfJ5FhFKKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cells can be used to check what Python version Colab is using.\n",
        "\n",
        "Unfortunately, the Python version used in the bash command can disagree with what is used by Python cells. This won't be an issue if you don't work with virtual environments or try to change versions.\n",
        "\n",
        "If you decide to change versions or use venvs for any reason, be careful!"
      ],
      "metadata": {
        "id": "Ykg3pK64isUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version # check what version of Python bash commands use"
      ],
      "metadata": {
        "id": "7Lz0Il-4I1Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys # check what version of Python the Python cells use\n",
        "sys.version"
      ],
      "metadata": {
        "id": "828teX_fiYWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll need to use the deep learning library [PyTorch](https://pytorch.org/docs/stable/index.html).\n",
        "\n",
        "We can ensure these are installed (or install them if not already installed) using the package manager `pip`. To be safe, use `pip3` in most cases - this ensures you are installing packages for use with Python 3.\n",
        "\n",
        "We will also use `google.colab`, which of course is pre-installed in Colab environments."
      ],
      "metadata": {
        "id": "jVi2syudBcGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 check torch # check that torch is installed - it should be!"
      ],
      "metadata": {
        "id": "Vhg6gSdACRlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If at any point other packages/libraries need to be installed, and Colab throws an error, use `!pip3 install package-name**`."
      ],
      "metadata": {
        "id": "cI_YTuHXQie8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting your Google Drive\n",
        "\n",
        "Colab allows you to access your files via Google Drive. This is useful when working with large datasets. Simply store them in a drive you have access to and you'll never have to download them again!\n",
        "\n",
        "Let's see how:"
      ],
      "metadata": {
        "id": "-rAGNlBYANrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Google Drive - will show up in the folder 'drive'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # this line will produce a pop-up"
      ],
      "metadata": {
        "id": "BCdGAfcvxrqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check the contents of your Drive as follows. You can also use the icon that looks like a folder on the left-hand side of the screen."
      ],
      "metadata": {
        "id": "VhfUUjg4AgZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List the contents of your Google Drive.\n",
        "!ls \"/content/drive/My Drive/\""
      ],
      "metadata": {
        "id": "awRcejNiyIjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "Before we get into PyTorch, we should load up some data so we have something to actually learn from.\n",
        "\n",
        "An easy-to-use dataset is the [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which consists of a set of measurements taken of many iris flowers paired with the flowers' species. It is readily available from [`sklearn`](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset)."
      ],
      "metadata": {
        "id": "D9Nq7m9GdMeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "data['data'].shape, data['target'].shape"
      ],
      "metadata": {
        "id": "EsenCIfVde39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`sklearn` formats the Iris dataset as two arrays.\n",
        "\n",
        "The first, called `'data'`, contains 150 rows and 4 columns. This is meant to be interpreted as 150 samples or instances of data, with each having 4 features (sepal length, sepal width, petal length, petal width):"
      ],
      "metadata": {
        "id": "3wbWQJSBd9FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['data'].shape)\n",
        "data['data'][:5] # look at 5 iris' measurements"
      ],
      "metadata": {
        "id": "3ZVKVm7mfIEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second, called `'target'` is a single vector of 150 entries, which we can interpret as 150 rows and 1 column. Note that there are three classes (0 for Setosa, 1 for Versicolour, or 2 for Virginica) for each 4-feature input:"
      ],
      "metadata": {
        "id": "r5rDCa7MfIol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('instances:', data['target'].shape)\n",
        "print('number of unique classes:', len(set(data['target'])))\n",
        "data['target'][:5] # first 5 are all class 0 (Setosa)"
      ],
      "metadata": {
        "id": "scvYdbBrdszU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Let's try and pass the data through a neural net!"
      ],
      "metadata": {
        "id": "seYyELhkjXYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing a model"
      ],
      "metadata": {
        "id": "lCdoNnJnlkzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch is used to build, train, and run deep neural networks. Because the `nn` (neural network) and `nn.functional` (common mathematical functions used in neural networks) submodules are used so often, it is common practice to do the following:"
      ],
      "metadata": {
        "id": "Tp6s2kfmZ51F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "q7cP0e6tlFdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a simple neural network and try to complete a forward pass. In PyTorch, a neural network is written as follows:\n",
        "\n",
        "- must be a Python `class`\n",
        "- must subclass (inherit from) `torch.nn.Module`\n",
        "- must have an `__init__()` method (a constructor), in which ALL trainable model parameters are defined\n",
        "- must have a `forward()` method that defines the behavior of the model (how an input is passed through the layers)"
      ],
      "metadata": {
        "id": "V3VLWgnZg6bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        # do whatever must be done for an nn.Module\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # 4D input, 3D output\n",
        "        self.linear = nn.Linear(4, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # just pass the input through the linear layer\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "Ltlz5RMOg4N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a very simple neural network. In fact, it's hard to even call it a neural network because it is not deep - it has only one layer. This is commonly called a lienar model.\n",
        "\n",
        "Nonetheless, let's try to see what it looks like. I'm going to add a seed as well, so that we all get the same numbers every time (**reproducibility**):"
      ],
      "metadata": {
        "id": "AtmIiF0ckDFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model = Net() # make an instance of the Net class we just wrote\n",
        "model # take a look"
      ],
      "metadata": {
        "id": "-n9DYU8TkgUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 input features, 3 output features. How many parameters?"
      ],
      "metadata": {
        "id": "hFFQ_IeDkrM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0\n",
        "\n",
        "# iterate over model's parameters\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    print(param, '\\n\\n\\n')\n",
        "    s += param.numel() # count elements in a tensor\n",
        "print('total parameters:', s)"
      ],
      "metadata": {
        "id": "i421ebtmkxAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 15 including the bias term. This model represents what's called an **affine** transformation (one that takes something like $\\mathbf{x}$ and maps it to something like $W\\mathbf{x}+\\mathbf{b}$:\n",
        "\n",
        "$$ W\\mathbf{x} + \\mathbf{b} = \\hat{\\mathbf{y}} $$\n",
        "\n",
        "$$ \\begin{bmatrix}\n",
        "    0.3823 & 0.4150 & -0.1171 & 0.4593 \\\\\n",
        "    -0.1096 & 0.1009 & -0.2434 & 0.2936 \\\\\n",
        "    0.4408 & -0.3668 & 0.4346 & 0.0936 \\\\\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "    x_1 \\\\\n",
        "    x_2 \\\\\n",
        "    x_3 \\\\\n",
        "    x_4 \\\\\n",
        "\\end{bmatrix} + \\begin{bmatrix}\n",
        "    0.3694 \\\\\n",
        "    0.0677 \\\\\n",
        "    0.2411 \\\\\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    \\hat{y}_1 \\\\\n",
        "    \\hat{y}_2 \\\\\n",
        "    \\hat{y}_3 \\\\\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "Alternatively, we can let the first column of $W$ be the bias vector, and assign $\\mathbf{x}$ a dummy feature in its first slot which is always 1:\n",
        "\n",
        "$$ W\\mathbf{x} = \\hat{\\mathbf{y}} $$\n",
        "\n",
        "$$ \\begin{bmatrix}\n",
        "    0.3694 & 0.3823 & 0.4150 & -0.1171 & 0.4593 \\\\\n",
        "    0.0677 & -0.1096 & 0.1009 & -0.2434 & 0.2936 \\\\\n",
        "    0.2411 & 0.4408 & -0.3668 & 0.4346 & 0.0936 \\\\\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "    1   \\\\\n",
        "    x_1 \\\\\n",
        "    x_2 \\\\\n",
        "    x_3 \\\\\n",
        "    x_4 \\\\\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    \\hat{y}_1 \\\\\n",
        "    \\hat{y}_2 \\\\\n",
        "    \\hat{y}_3 \\\\\n",
        "\\end{bmatrix} $$"
      ],
      "metadata": {
        "id": "ohtGOQzrk4g_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we wanted to make this model a bit more powerful, it would be smart to (1) have multiple layers and (2) use a non-linearity between them so the model can learn non-linear functions:"
      ],
      "metadata": {
        "id": "0ul3KvFKpIJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNet(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        # do whatever must be done for an nn.Module\n",
        "        super(DeepNet, self).__init__()\n",
        "\n",
        "        # 4D input, 3D output\n",
        "        self.linear1 = nn.Linear(4, 8) # 8 is arbitrary\n",
        "        self.linear2 = nn.Linear(8, 3) # must match here\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # pass through first layer\n",
        "        x = self.linear1(x)\n",
        "\n",
        "        # activation function, ReLU\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # pass through second layer\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "h-F-V7stnV5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_model = DeepNet()\n",
        "deep_model"
      ],
      "metadata": {
        "id": "D-VrA71Opu3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0\n",
        "\n",
        "# iterate over model's parameters\n",
        "for name, param in deep_model.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    print(param, end='\\n\\n')\n",
        "    s += param.numel() # count elements in a tensor\n",
        "print('total parameters:', s)"
      ],
      "metadata": {
        "id": "9gmNbK6CpoS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks something like this, without writing out all the parameters:\n",
        "\n",
        "$$ W_2\\sigma(W_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2 = \\hat{\\mathbf{y}} $$\n",
        "\n",
        "with:\n",
        "\n",
        "$$ W_2 \\in \\mathbb{R}^{3\\times8} $$\n",
        "$$ W_1 \\in \\mathbb{R}^{8\\times4} $$\n",
        "$$ \\mathbf{x} \\in \\mathbb{R}^{4\\times1} $$\n",
        "$$ \\mathbf{b}_1 \\in \\mathbb{R}^{8\\times1} $$\n",
        "$$ \\mathbf{b}_2 \\in \\mathbb{R}^{3\\times1} $$\n",
        "\n",
        "Note that an activation function such as ReLU operates element-wise, so $$X\\in\\mathbb{R}^{m\\times n} \\implies \\sigma(X)\\in\\mathbb{R}^{m\\times n}$$"
      ],
      "metadata": {
        "id": "Wu9_miB4XBaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward pass"
      ],
      "metadata": {
        "id": "nLPhke1aqEUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by changing our data to a [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html). PyTorch works only on these specialized arrays, which are easily constructed from Python lists, [`numpy.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) objects or the like."
      ],
      "metadata": {
        "id": "FFC5smktqI6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_data = torch.Tensor(data['data'])\n",
        "print('data shape:', tensor_data.shape)\n",
        "print('\\nsome rows in our data:', tensor_data[50:55], sep='\\n')"
      ],
      "metadata": {
        "id": "n8vJRXQRqaUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with other array-like objects, you can slice them to get rows or columns.\n",
        "\n",
        "Now let's try and pass a single row through the `DeepNet`:"
      ],
      "metadata": {
        "id": "3ROnUFRiqkTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # calling a model calls its forward() method\n",
        "input_tensor = tensor_data[50]\n",
        "output_tensor = deep_model(input_tensor)\n",
        "print('input:', input_tensor, sep='\\n')\n",
        "print('input shape:', input_tensor.shape, end='\\n\\n')\n",
        "print('output:', output_tensor, sep='\\n')\n",
        "print('output shape:', output_tensor.shape)\n",
        "print('\\nprediction:', output_tensor.argmax().item())\n",
        "print('    target:', data['target'][50])"
      ],
      "metadata": {
        "id": "POpl8w1lq0sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also pass many rows through at once by **batching**:"
      ],
      "metadata": {
        "id": "QN6hW6h5PcHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_input_tensor = tensor_data[50:55]\n",
        "batched_output_tensor = deep_model(batched_input_tensor)\n",
        "print('input:', batched_input_tensor, sep='\\n')\n",
        "print('input shape:', batched_input_tensor.shape, end='\\n\\n')\n",
        "print('output:', batched_output_tensor, sep='\\n')\n",
        "print('output shape:', batched_output_tensor.shape)\n",
        "print('\\nprediction:', batched_output_tensor.argmax(dim=1).numpy())\n",
        "print('    target:', data['target'][50:55])"
      ],
      "metadata": {
        "id": "I1vbC2BBPbrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model predicts 0 for everything, but should predict 1 for these flowers - what's wrong?\n",
        "\n",
        "Let's supply some trained weights:"
      ],
      "metadata": {
        "id": "a5zeKsvqbP5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_weights_path = '/content/drive/Shareddrives/COSC 3470 Spring 2025/trained-models/iris_model.pt'\n",
        "\n",
        "deep_model.load_state_dict(\n",
        "    torch.load(trained_weights_path)\n",
        ")\n",
        "batched_output_tensor = deep_model(batched_input_tensor)\n",
        "print('prediction:', batched_output_tensor.argmax(dim=1).numpy())\n",
        "print('    target:', data['target'][50:55])"
      ],
      "metadata": {
        "id": "9bD_7R3RbXXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hardware considerations\n",
        "\n",
        "Deep learning is computationally expensive. As such, it's good to be mindful of choices that will reduce your computational expense for financial, environmental, and efficiency reasons.\n",
        "\n",
        "One thing we can do to be faster is to use a GPU.\n",
        "\n",
        "GPUs are designed to perform vectorized linear algebra computations on arrays of numbers. These computations can be done much more efficiently than on a CPU. To take advantage of this, we must put all relevant objects on the GPU. This can be done using `.to(device_name)`. Using `'cuda'` as the device name will find the first GPU the system has detected."
      ],
      "metadata": {
        "id": "tcsZ4614rNuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# move the input and model to GPU for speed if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "input_tensor = input_tensor.to(device) # out of place - must reassign with =\n",
        "model.to(device) # models can be done in place\n",
        "output = model(input_tensor)\n",
        "output"
      ],
      "metadata": {
        "id": "vPorJayk1gk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell will confirm that both the input tensor and the model are on the same GPU if you're in a GPU runtime environment. Otherwise, both with be on CPU."
      ],
      "metadata": {
        "id": "trPmLZs9fIj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(next(model.parameters()).device)\n",
        "input_tensor.device"
      ],
      "metadata": {
        "id": "pg5I2ubQZUSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training vs inference practices"
      ],
      "metadata": {
        "id": "zSg0Wfi8sWPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, your model is in training mode. You can ensure training mode with:"
      ],
      "metadata": {
        "id": "jrBrdTq4sbBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deep_model.train()"
      ],
      "metadata": {
        "id": "03dUE_nPsfRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are done training and ready to:\n",
        "\n",
        "- evaluate on a dev set\n",
        "- evaluate on a test set\n",
        "- run inference on some data without updating your model's weights\n",
        "- deploy a model for some purpose\n",
        "\n",
        "You should do two things:"
      ],
      "metadata": {
        "id": "DNSnbjg9shnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Set eval mode (turns off dropout, batch normalization, and any other things that are only supposed to be active during training):"
      ],
      "metadata": {
        "id": "dxVhc69As2Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deep_model.eval()"
      ],
      "metadata": {
        "id": "Imn_AShds2-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Use the `with torch.no_grad()` context manager to avoid computation of gradients. This computation adds significantly to the time needed to process an input, and is unnecessary unless we wish to update the model's parameters."
      ],
      "metadata": {
        "id": "EGp20P_ms-HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output = deep_model(input_tensor)\n",
        "output"
      ],
      "metadata": {
        "id": "VPtGlBovswZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be sure to always switch back to training mode before you continue to train!"
      ],
      "metadata": {
        "id": "HR96XDp3tVzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More practice (if time)\n",
        "\n",
        "Take a look at this dataset of [hand-written digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits).\n",
        "\n",
        "Try to define a model architecture suited to these inputs and outputs."
      ],
      "metadata": {
        "id": "_01CJNTGp53L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits"
      ],
      "metadata": {
        "id": "n17ug7USp9Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SF9wIOHQqfas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus"
      ],
      "metadata": {
        "id": "0M8SsvJiMhKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "data['data'].shape, data['target'].shape"
      ],
      "metadata": {
        "id": "kD8_GImOMich"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(data['data'])\n",
        "y = torch.tensor(data['target'])\n",
        "\n",
        "# hold out rows 50-55\n",
        "X_test = X[50:55]\n",
        "y_test = y[50:55]\n",
        "\n",
        "X_train = torch.cat((X[:50], X[55:]))\n",
        "y_train = torch.cat((y[:50], y[55:]))"
      ],
      "metadata": {
        "id": "HiFaoIZ6MnvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crit = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "EcqJNnjfMsJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    opt.zero_grad()\n",
        "    loss = crit(model(X_train.float()), y_train)\n",
        "    loss.backward()\n",
        "    opt.step()"
      ],
      "metadata": {
        "id": "P0N1PokZMtlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(X_test.float()).argmax(dim=1).tolist(), y_test.tolist()"
      ],
      "metadata": {
        "id": "FGoCLbXDMwIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'iris_model.pt')"
      ],
      "metadata": {
        "id": "ChmSuWXhM0Pg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}