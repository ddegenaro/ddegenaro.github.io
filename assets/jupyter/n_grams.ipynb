{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj3Lt-nJnEEw"
      },
      "source": [
        "# $n$-gram Models\n",
        "\n",
        "An $n$-gram model is the simplest kind of language model. It is a *probabilistic* model of language. That is, it *assigns* probabilities to sequences of words.\n",
        "\n",
        "Think about the following sentence fragment:\n",
        "\n",
        "> **The cat sat on the _____**\n",
        "\n",
        "What word should fill in the blank?\n",
        "\n",
        "If you said\n",
        "\n",
        "> **mat**\n",
        "\n",
        "then we agree! But why do we agree?\n",
        "\n",
        "Well, you might argue that our internal language models have both assigned **mat** a high probability, *given the preceding context.*\n",
        "\n",
        "Because we considered 5 words to predict the 6th word, we may have accessed something in our minds that behaves like a 6-gram language model - that is, an $n$-gram model with $n=6$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al5vIHIiorQu"
      },
      "source": [
        "## Mathematical definition of probability\n",
        "\n",
        "An $n$-gram language model defines a *probability distribution* over all known words, given some preceding context.\n",
        "\n",
        "Let's define all these words mathematically, and very carefully!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6YXoEOupubY"
      },
      "source": [
        "### Probability distributions\n",
        "\n",
        "A probability distribution $p(x)$ is a *function* that takes an event, $x$, and outputs a probability $p(x)$. For instance, here is a probability distribution describing the process of flipping a fair coin:\n",
        "\n",
        "$$ p(x=\\text{HEADS}) = 1/2 $$\n",
        "$$ p(x=\\text{TAILS}) = 1/2 $$\n",
        "\n",
        "Easy enough, right? We expect half of all coin flips to be heads, and the other half to be tails. Notice that the probabilities are between 0 and 1, and that they sum to 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UZQYYZEwupb",
        "outputId": "fcaf90a5-87ba-4953-a6a2-c097b478820e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p(HEADS): 0.5\n",
            "p(TAILS): 0.5\n",
            "     sum: 1.0\n"
          ]
        }
      ],
      "source": [
        "def p_coin_flip(x):\n",
        "    if x == \"HEADS\":\n",
        "        return 1/2\n",
        "    elif x == \"TAILS\":\n",
        "        return 1/2\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "print('p(HEADS):', p_coin_flip(\"HEADS\"))\n",
        "print('p(TAILS):', p_coin_flip(\"TAILS\"))\n",
        "\n",
        "s = 0\n",
        "for outcome in [\"HEADS\", \"TAILS\"]:\n",
        "    s += p_coin_flip(outcome)\n",
        "print('     sum:', s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f22K0etEwt1s"
      },
      "source": [
        "Another simple example is rolling a fair die (like in a board game):\n",
        "\n",
        "$$ p(x=1) = 1/6 $$\n",
        "$$ p(x=2) = 1/6 $$\n",
        "$$ p(x=3) = 1/6 $$\n",
        "$$ p(x=4) = 1/6 $$\n",
        "$$ p(x=5) = 1/6 $$\n",
        "$$ p(x=6) = 1/6 $$\n",
        "\n",
        "There are six faces on the die, and we have defined $x$ to mean \"how many dots are on the side facing up after rolling it.\" All the sides are equally likely to end up face-up, so all six events have the same probability. Notice again that the probabilities are between 0 and 1, and that they sum to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_9KFOBYxTb9",
        "outputId": "cfb65abb-d945-4d54-8c62-8583d31ffcdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p(1): 0.16666666666666666\n",
            "p(2): 0.16666666666666666\n",
            "p(3): 0.16666666666666666\n",
            "p(4): 0.16666666666666666\n",
            "p(5): 0.16666666666666666\n",
            "p(6): 0.16666666666666666\n",
            " sum: 0.9999999999999999\n"
          ]
        }
      ],
      "source": [
        "def p_die6(x):\n",
        "    if x == 1:\n",
        "        return 1/6\n",
        "    elif x == 2:\n",
        "        return 1/6\n",
        "    elif x == 3:\n",
        "        return 1/6\n",
        "    elif x == 4:\n",
        "        return 1/6\n",
        "    elif x == 5:\n",
        "        return 1/6\n",
        "    elif x == 6:\n",
        "        return 1/6\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "print('p(1):', p_die6(1))\n",
        "print('p(2):', p_die6(2))\n",
        "print('p(3):', p_die6(3))\n",
        "print('p(4):', p_die6(4))\n",
        "print('p(5):', p_die6(5))\n",
        "print('p(6):', p_die6(6))\n",
        "\n",
        "s = 0\n",
        "for outcome in range(1, 7): # 1, 2, 3, 4, 5, 6\n",
        "    s += p_die6(outcome)\n",
        "print(' sum:', s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6qhUWVKq_Cx"
      },
      "source": [
        "### Properties that probability distributions must satisfy\n",
        "\n",
        "Let's call the set of all possible events $\\mathcal{X}$ (sometimes called the *support*). For a coin flip,\n",
        "\n",
        "$$ \\mathcal{X} = \\{\\text{HEADS}, \\text{TAILS}\\} $$\n",
        "\n",
        "For a die roll,\n",
        "\n",
        "$$ \\mathcal{X} = \\{1, 2, 3, 4, 5, 6\\} $$\n",
        "\n",
        "Then, we need $p(x)$ to satisfy two properties. First, probabilities are restricted in their possible values:\n",
        "\n",
        "$$ \\forall x \\in \\mathcal{X} : 0 \\leq p(x) \\leq 1 $$\n",
        "\n",
        "This means that for every possible outcome $x$, we need to define $x$'s probability to be between 0 and 1. A probability of 0 means it is impossible, while a probability of 1 means it is certain. Of course, nothing can be less likely than *impossible*, and nothing can be more likely than *certain!*\n",
        "\n",
        "Second, probabilities need to be compatible with each other:\n",
        "\n",
        "$$ \\sum_{x\\in\\mathcal{X}} p(x) = 1 $$\n",
        "\n",
        "This means that *something* must happen! If we consider every possible outcome, SOME outcome MUST happen - so the probabilities add up to 1. We are 100% certain that *something* will happen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu73H8CHx2GH",
        "outputId": "8f115c98-de9c-4ae8-cdb5-e12272db60c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         coin flip: True\n",
            "six-sided die roll: True\n"
          ]
        }
      ],
      "source": [
        "def is_valid_probability_distribution(p, outcomes):\n",
        "\n",
        "    \"\"\"\n",
        "    Check if a probability distribution `p` with support `outcomes` is valid.\n",
        "    \"\"\"\n",
        "\n",
        "    # sum up total probability - it should be exactly 1\n",
        "    s = 0\n",
        "\n",
        "    # consider all outcomes in the support\n",
        "    for outcome in outcomes:\n",
        "\n",
        "        # make sure the probability assigned is valid\n",
        "        if p(outcome) < 0 or p(outcome) > 1:\n",
        "            return False # if not, the distribution is invalid!\n",
        "\n",
        "        s += p(outcome) # if it is, keep going - add up the contribution\n",
        "\n",
        "    # check if the sum is close to 1 - we can have precision mistakes :(\n",
        "    return abs(s - 1) < 0.00001\n",
        "\n",
        "print(\n",
        "    '         coin flip:',\n",
        "    is_valid_probability_distribution(p_coin_flip, [\"HEADS\", \"TAILS\"])\n",
        ")\n",
        "\n",
        "print(\n",
        "    'six-sided die roll:',\n",
        "    is_valid_probability_distribution(p_die6, [1, 2, 3, 4, 5, 6])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoNpfzr2tFPY"
      },
      "source": [
        "### Conditional probability\n",
        "\n",
        "*Conditional probabilities* tell us about how a probability distribution changes depending on what information we know.\n",
        "\n",
        "Let's start by thinking about coin flips and dice.\n",
        "\n",
        "Imagine that we carry out the following process:\n",
        "- Flip a coin. Assign the outcome of the coin flip to the variable $x$.\n",
        "- If heads, roll a 4-sided die.\n",
        "- If tails, roll a 6-sided die.\n",
        "- Assign the outcome of the die roll to the variable $y$.\n",
        "\n",
        "Then, $p(x)$ looks familiar:\n",
        "\n",
        "$$ p(x=\\text{H}) = 1/2 $$\n",
        "$$ p(x=\\text{T}) = 1/2 $$\n",
        "\n",
        "What about $p(y)$? Things are a little more complicated here. It's a bit easier to start by talking about $p(y|x)$, read as \"the probability of $y$ given $x$.\"\n",
        "\n",
        "If the coin is heads, 5 and 6 become impossible, because we are rolling the 4-sided die:\n",
        "\n",
        "$$ p(y=1|x=\\text{H}) = 1/4 $$\n",
        "$$ p(y=2|x=\\text{H}) = 1/4 $$\n",
        "$$ p(y=3|x=\\text{H}) = 1/4 $$\n",
        "$$ p(y=4|x=\\text{H}) = 1/4 $$\n",
        "$$ p(y=5|x=\\text{H}) = 0 $$\n",
        "$$ p(y=6|x=\\text{H}) = 0 $$\n",
        "\n",
        "If the coin is tails, we have the 6-sided die:\n",
        "\n",
        "$$ p(y=1|x=\\text{T}) = 1/6 $$\n",
        "$$ p(y=2|x=\\text{T}) = 1/6 $$\n",
        "$$ p(y=3|x=\\text{T}) = 1/6 $$\n",
        "$$ p(y=4|x=\\text{T}) = 1/6 $$\n",
        "$$ p(y=5|x=\\text{T}) = 1/6 $$\n",
        "$$ p(y=6|x=\\text{T}) = 1/6 $$\n",
        "\n",
        "Notice how the *context* of the coin flip influenced our expectations regarding what numbers we might see? But we still don't quite know what $p(y)$ looks like..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts80X7u1AXD6"
      },
      "source": [
        "### Joint probability\n",
        "\n",
        "*Joint probabilities* tell us about how likely it is that *multiple events occur together.* We write this as $p(x,y)$, which is read \"probability of $x$ and $y$.\" In general,\n",
        "\n",
        "$$ p(x,y) = p(y|x) * p(x) $$\n",
        "\n",
        "Think about this. In order to get to the point where we roll the 4-sided die, we *first need to get heads* ($p(x=\\text{H})=1/2$) when we flip the coin. Therefore, we will only ever consider the 4-sided die half the time.\n",
        "\n",
        "Now, within this subset of the possible events that can happen, we can consider our $p(y|x=\\text{H})$ from above. The total probability of, for instance, getting heads and then rolling a 3 is the probability of getting heads (1/2) *times* the probability of *now getting a 3 given we just got heads (1/4).*\n",
        "\n",
        "In the above example, we have:\n",
        "\n",
        "$$ p(x=\\text{H},y=1) = p(y=1|x=\\text{H}) * p(x=\\text{H}) = 1/4 * 1/2 = 1/8 $$\n",
        "$$ p(x=\\text{H},y=2) = p(y=2|x=\\text{H}) * p(x=\\text{H}) = 1/4 * 1/2 = 1/8 $$\n",
        "$$ p(x=\\text{H},y=3) = p(y=3|x=\\text{H}) * p(x=\\text{H}) = 1/4 * 1/2 = 1/8 $$\n",
        "$$ p(x=\\text{H},y=4) = p(y=4|x=\\text{H}) * p(x=\\text{H}) = 1/4 * 1/2 = 1/8 $$\n",
        "$$ p(x=\\text{H},y=5) = p(y=5|x=\\text{H}) * p(x=\\text{H}) = 0 * 1/2 = 0 $$\n",
        "$$ p(x=\\text{H},y=6) = p(y=6|x=\\text{H}) * p(x=\\text{H}) = 0 * 1/2 = 0 $$\n",
        "\n",
        "$$ p(x=\\text{T},y=1) = p(y=1|x=\\text{T}) * p(x=\\text{T}) = 1/6 * 1/2 = 1/12 $$\n",
        "$$ p(x=\\text{T},y=2) = p(y=2|x=\\text{T}) * p(x=\\text{T}) = 1/6 * 1/2 = 1/12 $$\n",
        "$$ p(x=\\text{T},y=3) = p(y=3|x=\\text{T}) * p(x=\\text{T}) = 1/6 * 1/2 = 1/12 $$\n",
        "$$ p(x=\\text{T},y=4) = p(y=4|x=\\text{T}) * p(x=\\text{T}) = 1/6 * 1/2 = 1/12 $$\n",
        "$$ p(x=\\text{T},y=5) = p(y=5|x=\\text{T}) * p(x=\\text{T}) = 1/6 * 1/2 = 1/12 $$\n",
        "$$ p(x=\\text{T},y=6) = p(y=6|x=\\text{T}) * p(x=\\text{T}) = 1/6 * 1/2 = 1/12 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS-ems1aC6uk"
      },
      "source": [
        "Overall, we can calculate $p(y)$ by summing up all the different ways we can get each possible value of $y$. There are two ways to get a 1, for instance:\n",
        "- We can get heads and then roll a 1 on the 4-sided die (probability $1/2 * 1/4 = 1/8$)\n",
        "- We can get tails and then roll a 1 on the 6-sided die (probability $1/2 * 1/6 = 1/12$)\n",
        "\n",
        "$$ p(y=1) = 1/8 + 1/12 \\approx 0.208 $$\n",
        "$$ p(y=2) = 1/8 + 1/12 \\approx 0.208 $$\n",
        "$$ p(y=3) = 1/8 + 1/12 \\approx 0.208 $$\n",
        "$$ p(y=4) = 1/8 + 1/12 \\approx 0.208 $$\n",
        "$$ p(y=5) = 0 + 1/12 \\approx 0.083 $$\n",
        "$$ p(y=6) = 0 + 1/12 \\approx 0.083 $$\n",
        "\n",
        "Let's simulate this and see if it comes out that way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M4ugKvPnAQp",
        "outputId": "e35b8276-1be0-47c8-984b-83727383aadc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 0.202\n",
            "2 0.196\n",
            "3 0.232\n",
            "4 0.199\n",
            "5 0.085\n",
            "6 0.086\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# create a mapping that has inputs 1...6 and all outputs are 0\n",
        "p_y = dict.fromkeys([1, 2, 3, 4, 5, 6], 0)\n",
        "\n",
        "N = 1000 # number of observations we will make\n",
        "\n",
        "for i in range(N): # 0, ..., N-1\n",
        "    coin_flip = random.randint(0, 1) # randomly choose 0 or 1\n",
        "    if coin_flip == 0:\n",
        "        die_roll = random.randint(1, 4) # randomly choose 1, 2, 3, or 4\n",
        "    else:\n",
        "        die_roll = random.randint(1, 6) # randomly choose 1, ..., 6\n",
        "    p_y[die_roll] += 1 # count our observation\n",
        "\n",
        "for key, value in p_y.items():\n",
        "    print(key, value / N) # divide by N to get observed probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvaxzxBa1Zy0"
      },
      "source": [
        "## Learning a unigram model\n",
        "\n",
        "Coin flips and dice are boring. Let's talk about natural language.\n",
        "\n",
        "Let $p(w|c)$ mean the probability of observing a word $w$ given some preceding context $c$. $c$ can be anything we want - it can be the preceding word, the preceding 2 words, 3 words, ... or even nothing at all!\n",
        "\n",
        "Let's start from the simplest case, which is indeed the case where $c$ is nothing at all - this is called a *unigram* model. Why? Because every word is considered totally independently of any other - it is a 1-gram!\n",
        "\n",
        "Mathematically speaking, the unigram distribution is defined as follows. Consider a sequence of $n$ words:\n",
        "\n",
        "$$ w_1, w_2, w_3, ... w_{n-2}, w_{n-1}, w_n $$\n",
        "\n",
        "The unigram distribution is defined such that:\n",
        "\n",
        "$$ p(w_n | w_1, w_2, ... w_{n-1}) = p(w_n) $$\n",
        "\n",
        "That is, the preceding context is totally *irrelevant* - it makes no difference what words we have seen in the past!\n",
        "\n",
        "This will be a very bad model of language, obviously. Nonetheless, let's train one!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPzgOnLD3U9s"
      },
      "source": [
        "The [NLTK library](https://www.nltk.org/) is a great tool to explore this. Here is the Brown corpus, one of the first corpora developed for computational linguistics research:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE2GEbTQ1csN",
        "outputId": "576776be-bdeb-4588-9613-73cb66bf6840"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "try:\n",
        "    words = brown.words()\n",
        "except:\n",
        "    nltk.download('brown')\n",
        "    words = brown.words()\n",
        "\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwwBwd6v4sqe"
      },
      "source": [
        "Here is a simple way to define probabilities of words. Just count them up, and divide by the sum of all the words!\n",
        "\n",
        "For instance, in:\n",
        "\n",
        "> the cat sat on the mat\n",
        "\n",
        "We have 6 words, and 2 of them are \"the,\" so we'll assign $p(\\text{the}) = 2/6$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo2kbDF43KQ1",
        "outputId": "5a87bed9-3afc-4e30-da88-9d492d2e9e8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  The: 7258\n",
            "hello: 4\n"
          ]
        }
      ],
      "source": [
        "unigram_counts = {} # dictionary, a mapping\n",
        "\n",
        "for word in words: # iterate over all the words\n",
        "    if word in unigram_counts: # if we have already seen the word\n",
        "        unigram_counts[word] += 1 # add 1 to its count\n",
        "    else: # if this is a new word\n",
        "        unigram_counts[word] = 1 # add a new entry to the dictionary, value 1\n",
        "\n",
        "# we can access any word's count like this:\n",
        "print(f'  The: {unigram_counts[\"The\"]}')\n",
        "print(f'hello: {unigram_counts[\"hello\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3UsyFTE5c-Y"
      },
      "source": [
        "Let's now divide the counts by the sum of all the counts, giving us probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqN-U-tY5-vu"
      },
      "outputs": [],
      "source": [
        "# iterate over key-value pairs in dict\n",
        "for key, value in unigram_counts.items():\n",
        "    # re-assign the values with the normalized values\n",
        "    # len(words) is the number of words in the corpus\n",
        "    unigram_counts[key] = value / len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng8dbIpD4FH1",
        "outputId": "ba0d2701-040a-4cdc-d129-7177689e727f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  p(The): 0.06025790739171472\n",
            "p(hello): 8.611840246918683e-06\n"
          ]
        }
      ],
      "source": [
        "def p_unigram(word):\n",
        "    if word in unigram_counts:\n",
        "        return unigram_counts[word]\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "print(f'  p(The): {p_unigram(\"The\")+p_unigram(\"the\")}')\n",
        "print(f'p(hello): {p_unigram(\"hello\")+p_unigram(\"Hello\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo1kGoev6j1L"
      },
      "source": [
        "We can also confirm our probability distribution is valid! We can call `.keys()` on our dictionary to get the support (and `.values()` to get the probabilities). We can also cast them to `list` type for later convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38v2qswk56QN",
        "outputId": "05f7145a-d6ca-464c-e202-197904882747"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unigram_support = list(unigram_counts.keys())\n",
        "unigram_probs = list(unigram_counts.values())\n",
        "is_valid_probability_distribution(p_unigram, unigram_support)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw3qJ9hF63TY"
      },
      "source": [
        "Great! Let's try to generate some text from our unigram distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P02slPQG6i29"
      },
      "outputs": [],
      "source": [
        "def generate_unigrams(sequence_length: int):\n",
        "    sequence = [] # initialize empty sequence\n",
        "    for i in range(sequence_length): # sample this many words\n",
        "        choice = random.choices(\n",
        "            unigram_support, # choose from the support\n",
        "            weights=unigram_probs # given their probabilities\n",
        "        )\n",
        "        sequence += choice # add on to the sequence\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IQbnhWMY74Wi",
        "outputId": "1b127d82-cf97-40ed-ad37-46b1c0a137ed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a all a Administration one subdivision , in critical displayed of Forsythe and example walked on Report when . Minutes'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' '.join(generate_unigrams(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcv3wpYx80r_"
      },
      "source": [
        "Not very good, right?\n",
        "\n",
        "Let's try higher order $n$-grams..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3shq426B8-Yt"
      },
      "source": [
        "## Learning a bigram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fg-ijOi9H9D"
      },
      "source": [
        "It's not too hard to turn this corpus into a bigram model - we just need to be careful about:\n",
        "- indexing correctly\n",
        "- accessing counts correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZWCNxWA75VT"
      },
      "outputs": [],
      "source": [
        "# nested dictionary\n",
        "bigram_counts = {}\n",
        "\n",
        "# there will always be one fewer bigrams than unigrams\n",
        "for i in range(len(words) - 1):\n",
        "\n",
        "    # our bigram is \"word1 word2\"\n",
        "    word1, word2 = words[i], words[i+1]\n",
        "\n",
        "    # every word gets its own dictionary!\n",
        "    if word1 not in bigram_counts:\n",
        "        bigram_counts[word1] = {}\n",
        "\n",
        "    # if it's a new bigram, set up a count\n",
        "    if word2 not in bigram_counts[word1]:\n",
        "        bigram_counts[word1][word2] = 1\n",
        "    else: # otherwise just count\n",
        "        bigram_counts[word1][word2] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCLkCAmMKv1v",
        "outputId": "3c5068f6-c232-44ec-abed-0490beb82381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(bigram_counts['Fulton']['County'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEYCgSoyLA_e"
      },
      "source": [
        "So, the bigram \"Fulton County\" occurs 6 times in the Brown corpus. Great!\n",
        "\n",
        "How do we get probabilities from this dictionary of dictionaries?\n",
        "\n",
        "Well, it depends on what we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE79n8mpLLnn"
      },
      "source": [
        "### Joint probabilities from the counts\n",
        "\n",
        "Remember that the joint probability $p(x,y)$ tells us the overall probability of seeing $x$ and $y$ together, out of all possible combinations of $x$ and $y$.\n",
        "\n",
        "If we look at any particular count in our dictionary of dictionaries, we know how many times we observed that bigram. Of course, the sum of all the counts is the total number of bigrams we saw.\n",
        "\n",
        "So, we can divide the two to make a claim about \"what proportion of all bigrams are this particular bigram?\" That is our joint probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsDbjS7AK-ZC",
        "outputId": "72ca40ba-b2cd-486e-a436-ebbde272c2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.1671085979825885e-06\n",
            "5.1671085979825885e-06\n"
          ]
        }
      ],
      "source": [
        "s = 0 # total number of bigrams observed\n",
        "for word1 in bigram_counts:\n",
        "    for word2 in bigram_counts[word1]:\n",
        "        s += bigram_counts[word1][word2]\n",
        "\n",
        "bigram_joint_probs = {} # new dictionary of dictionaries\n",
        "for word1 in bigram_counts: # for each word1\n",
        "    bigram_joint_probs[word1] = {} # set up a new dictionary\n",
        "    for word2 in bigram_counts[word1]: # for each word2\n",
        "        # compute p(x,y) and store it\n",
        "        bigram_joint_probs[word1][word2] = bigram_counts[word1][word2] / s\n",
        "\n",
        "# p(Fulton County)\n",
        "print(bigram_joint_probs['Fulton']['County'])\n",
        "\n",
        "# earlier, we found that \"Fulton County\" occurs 6 times\n",
        "# we also know that a corpus of N words has N-1 bigrams:\n",
        "print(6 / (len(words) - 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jnbbOEOYwxQ"
      },
      "source": [
        "### Conditional probabilities from the counts\n",
        "\n",
        "That's all fine and dandy, but an $n$-gram model uses *conditional* probabilities. How do we get those?\n",
        "\n",
        "The right way to think about this is that each word gets *its own probability distribution.*\n",
        "\n",
        "Consider a single dictionary from the counts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoHKF65CSLAT",
        "outputId": "1e227f4a-9304-44c7-8fee-06f5081a41e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "County 6\n",
            "Superior 2\n",
            "legislators 2\n",
            "taxpayers 1\n",
            "ordinary's 1\n"
          ]
        }
      ],
      "source": [
        "i = 0 # easy way to just look at a few entries\n",
        "for word2 in bigram_counts['Fulton']:\n",
        "    print(word2, bigram_counts['Fulton'][word2])\n",
        "    i += 1\n",
        "    if i == 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5olVeL0myh0"
      },
      "source": [
        "We can think about this dictionary as defining a probability distribution *conditioned on having just seen the word \"Fulton.\"*\n",
        "\n",
        "We just need to normalize the counts *within each dictionary:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t2Sz5P9ZI-R",
        "outputId": "111fe48e-495b-42ec-c7a9-03701a779a3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.35294117647058826\n",
            "0.35294117647058826\n"
          ]
        }
      ],
      "source": [
        "bigram_cond_probs = {} # new dictionary of dictionaries\n",
        "for word1 in bigram_counts: # for each word1\n",
        "    bigram_cond_probs[word1] = {} # set up a new dictionary\n",
        "    s = 0\n",
        "    for word2 in bigram_counts[word1]: # for each word2\n",
        "        s += bigram_counts[word1][word2] # add up the counts\n",
        "    for word2 in bigram_counts[word1]: # for each word2\n",
        "        # compute p(x|y) and store it\n",
        "        bigram_cond_probs[word1][word2] = bigram_counts[word1][word2] / s\n",
        "\n",
        "# p(County|Fulton)\n",
        "print(bigram_cond_probs['Fulton']['County'])\n",
        "\n",
        "# sanity check! it should be:\n",
        "# count(Fulton County) / count(Fulton)\n",
        "count_fulton_county = bigram_counts['Fulton']['County']\n",
        "count_fulton = sum(bigram_counts['Fulton'].values())\n",
        "print(count_fulton_county / count_fulton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-YTvj8pn92w"
      },
      "source": [
        "Ok! Let's generate!\n",
        "\n",
        "We'll sample a starting word from our good-ole unigram model, and from there, we'll condition on whatever word we last sampled to produce our next word!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuwCnTLfoG8u"
      },
      "outputs": [],
      "source": [
        "def generate_bigrams(sequence_length: int):\n",
        "\n",
        "    sequence = generate_unigrams(1) # initialize sequence with a unigram gen.\n",
        "\n",
        "    for i in range(sequence_length - 1): # sample this many words\n",
        "\n",
        "        last_word_dict = bigram_cond_probs[sequence[-1]]\n",
        "\n",
        "        choice = random.choices(\n",
        "            list(last_word_dict.keys()), # choose from the support\n",
        "            weights=list(last_word_dict.values()) # given their probabilities\n",
        "        )\n",
        "        sequence += choice # add on to the sequence\n",
        "\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "etNEHGwlojGY",
        "outputId": "0e063b3a-3088-4553-d452-8503c809aa64"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'up good look at four hundred years , and affied unto him . `` I hold up to show a'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' '.join(generate_bigrams(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol-8Rut3orr3"
      },
      "source": [
        "While this sentence probably didn't make much sense - parts of it are probably meaningful, right!? Isn't that odd???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ScH9Yueo2AZ"
      },
      "source": [
        "## Higher-order models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJTBth3Vq8lb"
      },
      "source": [
        "3-grams?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmGWsqhYo6Bj"
      },
      "outputs": [],
      "source": [
        "trigram_counts = {}\n",
        "\n",
        "# count\n",
        "for i in range(len(words) - 2):\n",
        "\n",
        "    bigram_context = (words[i], words[i + 1])\n",
        "\n",
        "    if bigram_context not in trigram_counts:\n",
        "        trigram_counts[bigram_context] = {}\n",
        "\n",
        "    third_word = words[i + 2]\n",
        "\n",
        "    if third_word not in trigram_counts[bigram_context]:\n",
        "        trigram_counts[bigram_context][third_word] = 1\n",
        "    else:\n",
        "        trigram_counts[bigram_context][third_word] += 1\n",
        "\n",
        "# normalize\n",
        "for bigram_context in trigram_counts:\n",
        "    s = 0\n",
        "    for third_word in trigram_counts[bigram_context]:\n",
        "        s += trigram_counts[bigram_context][third_word]\n",
        "    for third_word in trigram_counts[bigram_context]:\n",
        "        trigram_counts[bigram_context][third_word] /= s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDSZjgfUphbe"
      },
      "outputs": [],
      "source": [
        "def generate_trigrams(sequence_length: int):\n",
        "\n",
        "    sequence = generate_bigrams(2)\n",
        "\n",
        "    for i in range(sequence_length - 2):\n",
        "\n",
        "        bigram_context = (sequence[-2], sequence[-1])\n",
        "\n",
        "        last_word_dict = trigram_counts\n",
        "\n",
        "        choice = random.choices(\n",
        "            list(last_word_dict[bigram_context].keys()),\n",
        "            weights=list(last_word_dict[bigram_context].values())\n",
        "        )\n",
        "\n",
        "        sequence += choice\n",
        "\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gUmq5YpiqnI5",
        "outputId": "51f18ef8-267a-405d-a909-0741ae2969a4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'of Af . Thus if the revenues from any kind of case studies of fluids for hydraulically operated equipment ,'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' '.join(generate_trigrams(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XN6LsOeq-s2"
      },
      "source": [
        "4-grams???"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ql90250qo5I"
      },
      "outputs": [],
      "source": [
        "four_gram_counts = {}\n",
        "\n",
        "# count\n",
        "for i in range(len(words) - 3):\n",
        "\n",
        "    trigram_context = (words[i], words[i + 1], words[i + 2])\n",
        "\n",
        "    fourth_word = words[i + 3]\n",
        "\n",
        "    if trigram_context not in four_gram_counts:\n",
        "        four_gram_counts[trigram_context] = {}\n",
        "\n",
        "    if fourth_word not in four_gram_counts[trigram_context]:\n",
        "        four_gram_counts[trigram_context][fourth_word] = 1\n",
        "    else:\n",
        "        four_gram_counts[trigram_context][fourth_word] += 1\n",
        "\n",
        "for trigram_context in four_gram_counts:\n",
        "    s = 0\n",
        "    for fourth_word in four_gram_counts[trigram_context]:\n",
        "        s += four_gram_counts[trigram_context][fourth_word]\n",
        "\n",
        "    for fourth_word in four_gram_counts[trigram_context]:\n",
        "        four_gram_counts[trigram_context][fourth_word] /= s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxhh0h1krG1j"
      },
      "outputs": [],
      "source": [
        "def generate_four_grams(sequence_length: int):\n",
        "\n",
        "    sequence = generate_trigrams(3)\n",
        "\n",
        "    for i in range(sequence_length - 3):\n",
        "\n",
        "        trigram_context = (sequence[-3], sequence[-2], sequence[-1])\n",
        "\n",
        "        last_word_dict = four_gram_counts\n",
        "\n",
        "        choice = random.choices(\n",
        "            list(last_word_dict[trigram_context].keys()),\n",
        "            weights=list(last_word_dict[trigram_context].values())\n",
        "        )\n",
        "\n",
        "        sequence += choice\n",
        "\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xux9HcnYrM3L",
        "outputId": "3b1d00d3-547f-464c-89e5-22e313297e98"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Iliad has two words for the Hound Of Heaven's Pursuit ) by judicial fiat . They didn't . The Department's\""
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' '.join(generate_four_grams(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzJBdJU1rQLg"
      },
      "source": [
        "Wow. This looks like actual text, right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-h3AFkeri6a"
      },
      "source": [
        "## Concluding Remarks\n",
        "\n",
        "$n$-grams are really cool, and are still in use for research purposes all the time! I am currently working on research regarding how well neural networks (like ChatGPT) can learn different $n$-gram distributions!\n",
        "\n",
        "They are also basically how ChatGPT works, believe it or not! Generative transformer models like ChatGPT use a very complex statistical framework that is more robust at modeling this behavior than simply counting.\n",
        "\n",
        "What LLMs do is try to learn the right probability distribution (each dictionary within a dictionary) for EVERY possible context! There are many clever mathematical and engineering breakthroughs that have happened that allow this to be possible, and I won't cover them here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLm-gU2jsgy-"
      },
      "source": [
        "## Discussion Questions\n",
        "\n",
        "- Do you think that humans are $n$-gram models? Why or why not?\n",
        "- Were you surprised by how well $n$-grams seem to mimic real text? Or do you think they are bad at this?\n",
        "- Many linguists find it upsetting that simple statistical models are so accurate. Others take comfort in the fact that statistical models struggle to accurately predict certain phenomena. Do you lean one way or the other after this exercise?\n",
        "- Did you enjoy this exercise? What questions do you still have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXpWFSBGucsM"
      },
      "source": [
        "## Things for you to explore...\n",
        "\n",
        "### ...if you are new to Python:\n",
        "\n",
        "[Think Python](https://greenteapress.com/wp/think-python-2e/?authuser=2) is an excellent, free book to learn Python.\n",
        "\n",
        "[NLTK](https://www.nltk.org/book/), the Python library we used, has a short book which I have not read all the way through, but which has some nice tutorials.\n",
        "\n",
        "### ...if you are interested in machine learning and deep learning:\n",
        "\n",
        "The [scikit-learn](https://scikit-learn.org/stable/user_guide.html) Python library has just about every ML algorithm you could possibly dream of, along with great tutorials.\n",
        "\n",
        "[PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html) is probably my favorite Python library ever. It is used to build deep learning models, and I use it *constantly* for all sorts of fun research and experiments.\n",
        "\n",
        "[HuggingFace](https://huggingface.co/docs/transformers/quicktour) is a great collection of Python libraries with open-source AI models, datasets, evaluation metrics, and lots more.\n",
        "\n",
        "### ...if you are interested in natural language processing (NLP):\n",
        "\n",
        "[Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/?authuser=2) is the bible of NLP. It is a free book, and is regularly updated. This notebook covers a lot of the same material as Chapter 3.\n",
        "\n",
        "[spaCy](https://spacy.io/usage/spacy-101) and [Stanza (Stanford's NLP tools)](https://stanfordnlp.github.io/stanza/) are excellent for performing \"classic\" NLP tasks.\n",
        "\n",
        "### ...if you want to learn about cutting-edge research\n",
        "\n",
        "I like [Google Scholar](https://scholar.google.com/) the best to look up papers. A lot of the excellent NLP work is published in [ACL venues](https://aclanthology.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99_uO2ZhxZ8w"
      },
      "source": [
        "And all that is the tip of the iceberg. If you are ever curious about:\n",
        "- computer science\n",
        "- linguistics\n",
        "- mathematics\n",
        "- physics\n",
        "- graduate school (master's or PhD)\n",
        "- Georgetown\n",
        "\n",
        "Feel free to reach out! drd92@georgetown.edu will be my email for at least the next 5 years while I complete my PhD."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
