{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT's word embeddings\n",
    "\n",
    "BERT is one of the seminal works in natural language processing (NLP). A team of Google researchers trained BERT to encode sequences of text tokens (sub-words) into useful numerical representations. These representations could then be used for many important NLP tasks, like question answering or logical inference.\n",
    "\n",
    "Models like BERT build on earlier work such as word2vec and GloVe, which are algorithms for learning numerical vectors as representations of words.\n",
    "\n",
    "In this notebook, I look at the static word embeddings learned by BERT. That is, BERT has learned a vector for every single word it knows (the bottom layer of the model, before being passed through the encoder layers to produce contextualized representations) - where do these vectors live in relation to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "os.makedirs('../plotly', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = AutoTokenizer.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\",\n",
    ").vocab\n",
    "words_in_order = sorted(vocab.keys(), key = lambda x: vocab[x])\n",
    "\n",
    "X = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ").bert.embeddings.word_embeddings.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = (2, 3, 4)\n",
    "\n",
    "pcas = dict.fromkeys(dims)\n",
    "\n",
    "for dim in dims:\n",
    "    pca = PCA(n_components=dim)\n",
    "    pcas[dim] = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dict.fromkeys(dims)\n",
    "\n",
    "for dim in dims:\n",
    "    d = {\n",
    "        'word': words_in_order\n",
    "    }\n",
    "    for i in range(1, dim+1):\n",
    "        d[f'pc{i}'] = pcas[dim][:, i-1].tolist()\n",
    "        \n",
    "    dfs[dim] = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_dict = dict(\n",
    "    orientation='h',\n",
    "    y=-0.15,\n",
    ")\n",
    "\n",
    "marker_dict = dict(\n",
    "    size=3,\n",
    "    opacity=0.4,\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    margin = go.layout.Margin(\n",
    "        b=20,\n",
    "        t=50,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    dfs[2],\n",
    "    x='pc1',\n",
    "    y='pc2',\n",
    "    title=f'BERT: PCA in 2 dimensions',\n",
    "    hover_data={'word': True, 'pc1': False, 'pc2': False},\n",
    "    height=450,\n",
    "    width=800\n",
    ")\n",
    "fig.update_layout(legend=legend_dict, margin=layout.margin, title_x=0.5)\n",
    "fig.update_traces(marker=marker_dict)\n",
    "fig.show()\n",
    "fig.write_html('../plotly/bert_pca_2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    dfs[3],\n",
    "    x='pc1',\n",
    "    y='pc2',\n",
    "    z='pc3',\n",
    "    title=f'BERT: PCA in 3 dimensions',\n",
    "    hover_data={'word': True, 'pc1': False, 'pc2': False, 'pc3': False},\n",
    "    height=450,\n",
    "    width=800\n",
    ")\n",
    "fig.update_layout(legend=legend_dict, margin=layout.margin, title_x=0.5)\n",
    "fig.update_traces(marker=marker_dict)\n",
    "fig.show()\n",
    "fig.write_html('../plotly/bert_pca_3.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    dfs[3],\n",
    "    x='pc1',\n",
    "    y='pc2',\n",
    "    color='pc3',\n",
    "    title=f'BERT: PCA in 3 dimensions',\n",
    "    hover_data={'word': True, 'pc1': False, 'pc2': False, 'pc3': False},\n",
    "    height=450,\n",
    "    width=800\n",
    ")\n",
    "fig.update_layout(legend=legend_dict, margin=layout.margin, title_x=0.5)\n",
    "fig.update_traces(marker=marker_dict)\n",
    "fig.show()\n",
    "fig.write_html('../plotly/bert_pca_3_color.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    dfs[4],\n",
    "    x='pc1',\n",
    "    y='pc2',\n",
    "    z='pc3',\n",
    "    color='pca4',\n",
    "    title=f'BERT: PCA in 4 dimensions',\n",
    "    hover_data={'word': True, 'pc1': False, 'pc2': False, 'pc3': False, 'pc4': False},\n",
    "    height=450,\n",
    "    width=800\n",
    ")\n",
    "fig.update_layout(legend=legend_dict, margin=layout.margin, title_x=0.5)\n",
    "fig.update_traces(marker=marker_dict)\n",
    "fig.show()\n",
    "fig.write_html('../plotly/bert_pca_4.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423/) (Devlin et al., NAACL 2019)\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) (Mikolov et al., ICLR 2013)\n",
    "- [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162.pdf) (Pennington et al., EMNLP 2014)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
