% @inproceedings{degenaro2025fortify,
%   title = {FORTIFY: Generative Model Fine-tuning with ORPO for ReTrieval Expansion of InFormal NoisY Text},
%   author = {DeGenaro, Dan and Yang, Eugene and King, Nolan and Etter, David and Carpenter, Cameron and Sanders, Kate and Martin, Alexander and Murray, Kenton and Kriz, Reno},
%   booktitle = {UNDER REVIEW},
%   year = {2025},
%   selected = {false}
% }

@inproceedings{degenaro-lupicki-2024-experiments,
    title = "Experiments in Mamba Sequence Modeling and {NLLB}-200 Fine-Tuning for Low Resource Multilingual Machine Translation",
    author = "DeGenaro, Dan  and
      Lupicki, Tom",
    editor = "Mager, Manuel  and
      Ebrahimi, Abteen  and
      Rijhwani, Shruti  and
      Oncevay, Arturo  and
      Chiruzzo, Luis  and
      Pugh, Robert  and
      von der Wense, Katharina",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.americasnlp-1.22/",
    doi = "10.18653/v1/2024.americasnlp-1.22",
    pages = "188--194",
    pdf = {https://aclanthology.org/2024.americasnlp-1.22.pdf},
    google_scholar_id = {u-x6o8ySG0sC},
    doi = {10.18653/v1/2024.americasnlp-1.22},
    url = {https://aclanthology.org/2024.americasnlp-1.22/},
    selected = {true},
    bibtex_show = {true},
    abbr = {NAACL},
    abstract = "This paper presents DC{\_}DMV`s submission to the AmericasNLP 2024 Shared Task 1: Machine Translation Systems for Indigenous Languages. Our submission consists of two multilingual approaches to building machine translation systems from Spanish to eleven Indigenous languages: fine-tuning the 600M distilled variant of NLLB-200, and an experiment in training from scratch a neural network using the Mamba State Space Modeling architecture. We achieve the best results on the test set for a total of 4 of the language pairs between two checkpoints by fine-tuning NLLB-200, and outperform the baseline score on the test set for 2 languages."
}