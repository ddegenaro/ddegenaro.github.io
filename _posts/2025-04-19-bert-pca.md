---
layout: post
title: Principal component analysis of BERT's static embeddings
date: 2025-04-19 12:00:00
description: Principal component analysis (PCA) is a simple algorithm that compresses high-dimensional vectors into a low-dimensional space. It does this by creating an orthogonal basis in the low-dimensional space using the directions along which the data has maximum variance in the original high-dimensional space.
tags: language-modeling
categories: work
giscus_comments: false
related_posts: true
---

## BERT's word embeddings

BERT is one of the seminal works in natural language processing (NLP). A team of Google researchers trained BERT to encode sequences of text tokens (sub-words) into useful numerical representations. These representations could then be used for many important NLP tasks, like question answering or logical inference.

Models like BERT build on earlier work such as word2vec and GloVe, which are algorithms for learning numerical vectors as representations of words.

In this notebook, I look at the static word embeddings learned by BERT. That is, BERT has learned a vector for every single word it knows (the bottom layer of the model, before being passed through the encoder layers to produce contextualized representations) - where do these vectors live in relation to each other?

### Figures

Below are a few projections of BERT's static embedding space.

<div class="l-page">
  <iframe src="{{ '/assets/plotly/bert_pca_2.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<div class="l-page">
  <iframe src="{{ '/assets/plotly/bert_pca_3.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<div class="l-page">
  <iframe src="{{ '/assets/plotly/bert_pca_3_color.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<div class="l-page">
  <iframe src="{{ '/assets/plotly/bert_pca_4.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

### Notebook

Below is a notebook you can use to visualize them yourself.

<a href="https://raw.githubusercontent.com/ddegenaro/ddegenaro.github.io/main/assets/jupyter/pca.ipynb" download>Download this Jupyter notebook</a>

{::nomarkdown}
{% assign jupyter_path = "assets/jupyter/pca.ipynb" | relative_url %}
{% capture notebook_exists %}{% file_exists assets/jupyter/pca.ipynb %}{% endcapture %}
{% if notebook_exists == "true" %}
{% jupyter_notebook jupyter_path %}
{% else %}

<p>Sorry, the notebook you are looking for does not exist.</p>
{% endif %}
{:/nomarkdown}
