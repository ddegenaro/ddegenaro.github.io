<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ddegenaro.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ddegenaro.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-30T05:59:13+00:00</updated><id>https://ddegenaro.github.io/feed.xml</id><title type="html">blank</title><subtitle>Academic stuff. </subtitle><entry><title type="html">Principal component analysis of BERT’s static embeddings</title><link href="https://ddegenaro.github.io/blog/2025/bert-pca/" rel="alternate" type="text/html" title="Principal component analysis of BERT’s static embeddings"/><published>2025-04-19T12:00:00+00:00</published><updated>2025-04-19T12:00:00+00:00</updated><id>https://ddegenaro.github.io/blog/2025/bert-pca</id><content type="html" xml:base="https://ddegenaro.github.io/blog/2025/bert-pca/"><![CDATA[<h2 id="berts-word-embeddings">BERT’s word embeddings</h2> <p>BERT is one of the seminal works in natural language processing (NLP). A team of Google researchers trained BERT to encode sequences of text tokens (sub-words) into useful numerical representations. These representations could then be used for many important NLP tasks, like question answering or logical inference.</p> <p>Models like BERT build on earlier work such as word2vec and GloVe, which are algorithms for learning numerical vectors as representations of words.</p> <p>In this notebook, I look at the static word embeddings learned by BERT. That is, BERT has learned a vector for every single word it knows (the bottom layer of the model, before being passed through the encoder layers to produce contextualized representations) - where do these vectors live in relation to each other?</p> <h3 id="figures">Figures</h3> <p>Below are a few projections of BERT’s static embedding space.</p> <div class="l-page"> <iframe src="/assets/plotly/bert_pca_2.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="l-page"> <iframe src="/assets/plotly/bert_pca_3.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="l-page"> <iframe src="/assets/plotly/bert_pca_3_color.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="l-page"> <iframe src="/assets/plotly/bert_pca_4.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <h3 id="notebook">Notebook</h3> <p>Below is a notebook you can use to visualize them yourself.</p> <p><a href="/assets/downloads/pca.ipynb" download="pca.ipynb">Download this Jupyter notebook</a></p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/pca.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="work"/><category term="language-modeling"/><summary type="html"><![CDATA[Principal component analysis (PCA) is a simple algorithm that compresses high-dimensional vectors into a low-dimensional space. It does this by creating an orthogonal basis in the low-dimensional space using the directions along which the data has maximum variance in the original high-dimensional space.]]></summary></entry><entry><title type="html">Creating a distribution with a specific entropy using PyTorch</title><link href="https://ddegenaro.github.io/blog/2025/entropy-optimization/" rel="alternate" type="text/html" title="Creating a distribution with a specific entropy using PyTorch"/><published>2025-03-26T12:00:00+00:00</published><updated>2025-03-26T12:00:00+00:00</updated><id>https://ddegenaro.github.io/blog/2025/entropy-optimization</id><content type="html" xml:base="https://ddegenaro.github.io/blog/2025/entropy-optimization/"><![CDATA[<p><a href="/assets/downloads/entropy_opt.ipynb" download="entropy_opt.ipynb">Download this Jupyter notebook</a></p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/entropy_opt.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="work"/><category term="information-theory"/><category term="pytorch"/><summary type="html"><![CDATA[Using an optimizer to play with the statistical properties of a distribution.]]></summary></entry><entry><title type="html">Lecture on n-grams</title><link href="https://ddegenaro.github.io/blog/2025/n-grams/" rel="alternate" type="text/html" title="Lecture on n-grams"/><published>2025-03-21T12:00:00+00:00</published><updated>2025-03-21T12:00:00+00:00</updated><id>https://ddegenaro.github.io/blog/2025/n-grams</id><content type="html" xml:base="https://ddegenaro.github.io/blog/2025/n-grams/"><![CDATA[<p><a href="/assets/downloads/n_grams.ipynb" download="n_grams.ipynb">Download this Jupyter notebook</a></p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/n_grams.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="work"/><category term="teaching"/><category term="language-modeling"/><summary type="html"><![CDATA[Guest lecture on n-grams delivered virtually at Walter Payton College Prepatory High School in Chicago, IL.]]></summary></entry><entry><title type="html">LaTeX workshop</title><link href="https://ddegenaro.github.io/blog/2024/latex-workshop/" rel="alternate" type="text/html" title="LaTeX workshop"/><published>2024-09-20T12:00:00+00:00</published><updated>2024-09-20T12:00:00+00:00</updated><id>https://ddegenaro.github.io/blog/2024/latex-workshop</id><content type="html" xml:base="https://ddegenaro.github.io/blog/2024/latex-workshop/"><![CDATA[]]></content><author><name></name></author><category term="work"/><category term="teaching"/><category term="latex"/><summary type="html"><![CDATA[Slides for a workshop I gave on general LaTeX usage.]]></summary></entry><entry><title type="html">Getting started with PyTorch in Google Colab</title><link href="https://ddegenaro.github.io/blog/2024/getting-started-pytorch/" rel="alternate" type="text/html" title="Getting started with PyTorch in Google Colab"/><published>2024-09-18T12:00:00+00:00</published><updated>2024-09-18T12:00:00+00:00</updated><id>https://ddegenaro.github.io/blog/2024/getting-started-pytorch</id><content type="html" xml:base="https://ddegenaro.github.io/blog/2024/getting-started-pytorch/"><![CDATA[<p><a href="/assets/downloads/get_started.ipynb" download="get_started.ipynb">Download this Jupyter notebook</a></p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/get_started.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="work"/><category term="teaching"/><category term="pytorch"/><summary type="html"><![CDATA[A brief introduction to using PyTorch in Colab. This notebook is adapted from some guest lectures I gave for courses for which I served as a TA.]]></summary></entry><entry><title type="html">Training a CNN on the MNIST dataset</title><link href="https://ddegenaro.github.io/blog/2024/mnist-cnn/" rel="alternate" type="text/html" title="Training a CNN on the MNIST dataset"/><published>2024-09-18T12:00:00+00:00</published><updated>2024-09-18T12:00:00+00:00</updated><id>https://ddegenaro.github.io/blog/2024/mnist-cnn</id><content type="html" xml:base="https://ddegenaro.github.io/blog/2024/mnist-cnn/"><![CDATA[<p><a href="/assets/downloads/mnist_cnn.ipynb" download="mnist_cnn.ipynb">Download this Jupyter notebook</a></p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/mnist_cnn.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="work"/><category term="teaching"/><category term="pytorch"/><category term="vision"/><summary type="html"><![CDATA[Code that trains a convolutional neural network to recognize hand-written digits. This notebook is adapted from some guest lectures I gave for courses for which I served as a TA.]]></summary></entry><entry><title type="html">Shannon information</title><link href="https://ddegenaro.github.io/blog/2024/shannon-information/" rel="alternate" type="text/html" title="Shannon information"/><published>2024-09-10T12:00:00+00:00</published><updated>2024-09-10T12:00:00+00:00</updated><id>https://ddegenaro.github.io/blog/2024/shannon-information</id><content type="html" xml:base="https://ddegenaro.github.io/blog/2024/shannon-information/"><![CDATA[<p><a href="/assets/downloads/shannon_information.ipynb" download="shannon_information.ipynb">Download this Jupyter notebook</a></p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/shannon_information.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="work"/><category term="information-theory"/><category term="language-modeling"/><summary type="html"><![CDATA[A brief introduction to Shannon information and its applications to language and language modelings. I put this notebook together as my contribution to a seminar in which every student was required to present on a technical topic.]]></summary></entry></feed>